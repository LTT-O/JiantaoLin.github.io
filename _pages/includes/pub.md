# üìù Publications 
## 3D Vision and Generative Models

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2025</div><img src='images/kiss_teaser.png' alt="kiss3dgen" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

#### **Kiss3DGen: Repurposing Image Diffusion Models for 3D Asset Generation**  
[**üìÑ Paper (arXiv)**](https://arxiv.org/pdf/2503.01370)   \\
**Jiantao Lin\***, Xin Yang\*, Meixi Chen\*, Yingjie Xu, Dongyu Yan, Leyi Wu, Xinli Xu, Lie Xu, Shunsi Zhang, Ying-Cong Chen

[**Project Page**](https://ltt-o.github.io/Kiss3dgen.github.io) | [**Code**](https://github.com/EnVision-Research/Kiss3DGen) | [**Demo**](https://envision-research.hkust-gz.edu.cn/kiss3dgen/)
  - This work explores utilizing 2D diffusion models for 3D asset generation.
  - Improves quality and diversity in 3D shape synthesis.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2024</div><img src='images/prm_teaser.png' alt="prm" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

#### **PRM: Photometric Stereo-based Large Reconstruction Model**  
[**üìÑ Paper (arXiv)**](https://arxiv.org/pdf/2412.07371)   \\
Wenhang Ge*, **Jiantao Lin\***, Jiawei Feng, Guibao Shen, Tao Hu, Xinli Xu, Ying-Cong Chen

[**Project Page**](https://wenhangge.github.io/PRM/) | [**Code**](https://github.com/g3956/PRM) | [**Demo**](https://huggingface.co/spaces/LTT/PRM)
  - A large-scale photometric stereo reconstruction framework.
  - Enhances lighting-aware 3D shape recovery with high fidelity.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/luciddreamer_teaser.jpg' alt="luciddreamer" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

#### **LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching**  
[**üìÑ Paper (arXiv)**](https://arxiv.org/abs/2311.11284) <strong><span class='show_paper_citations' data='ri-snP0AAAAJ:u5HHmVD_uO8C'></span><strong>   \\
Yixun Liang\*, Xin Yang\*, **Jiantao Lin**, Haodong Li, Xiaogang Xu, Ying-Cong Chen

[**Code**](https://github.com/EnVision-Research/LucidDreamer)  
  - Proposes a novel interval score matching approach for text-to-3D generation.
  - Significantly improves 3D shape realism and fidelity.
</div>
</div>

## 2D Vision and Generative Models

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2024</div><img src='images/flexgen_teaser.png' alt="flexgen" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

#### **FlexGen: Flexible Multi-View Generation from Text and Image Inputs**  
[**üìÑ Paper (arXiv)**](https://arxiv.org/abs/2410.10745)   \\
Xinli Xu\*, Wenhang Ge\*, **Jiantao Lin\***, Jiawei Feng, Lie Xu, Hanfeng Zhao, Shunsi Zhang, Ying-Cong Chen

[**Project Page**](https://xxu068.github.io/flexgen.github.io/)  
  - A multi-view generation model that fuses text and image inputs.
  - Enables controllable and high-quality multi-view synthesis.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2024</div><img src='images/sg_teaser.jpg' alt="sg-adapter" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

#### **SG-Adapter: Enhancing Text-to-Image Generation with Scene Graph Guidance**  
[**üìÑ Paper (arXiv)**](https://arxiv.org/abs/2405.15321)   \\
Guibao Shen\*, Luozhou Wang\*, **Jiantao Lin**, Wenhang Ge, Chaozhe Zhang, Xin Tao, Yuan Zhang, Pengfei Wan, Zhongyuan Wang, Guangyong Chen, Yijun Li, Ying-Cong Chen.
  - Introduces scene graph constraints into text-to-image generation.
  - Improves structure and semantic consistency in generated images.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Pattern Recogn. Lett 2024</div><img src='images/webly_teaser.jpg' alt="webly-fg" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

#### **Graph Representation and Prototype Learning for Webly Supervised Fine-Grained Image Recognition**  
[**üìÑ Paper (Pattern Recognition Letters)**](https://www.sciencedirect.com/science/article/abs/pii/S0167865524001405)   \\
**Jiantao Lin**, Tianshui Chen, Ying-Cong Chen, Zhijing Yang, Yuefang Gao
  - Proposes a novel graph-based method for fine-grained image recognition under weak supervision.
  - Learns better category structures for challenging datasets.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICPR 2022</div><img src='images/CGLRL_teaser.png' alt="global-local" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

#### **Learning Consistent Global-Local Representation for Cross-Domain Facial Expression Recognition**  
[**üìÑ Paper (ICPR 2022)**](https://ieeexplore.ieee.org/abstract/document/9956069)   \\
Yuhao Xie, Yuefang Gao, **Jiantao Lin**, Tianshui Chen

[**Code**](https://github.com/yao-papercodes/CGLRL)  
  - Introduces a domain-adaptive method for cross-domain facial expression recognition.
  - Bridges the gap between different facial datasets through global-local feature alignment.
</div>
</div>