# üìù Publications 
## 3D Generation/Reconstruction

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2025</div><img src='images/kiss_teaser.png' alt="kiss3dgen" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

#### **Kiss3DGen: Repurposing Image Diffusion Models for 3D Asset Generation**  
[**üìÑ Paper (arXiv)**](https://arxiv.org/pdf/2503.01370)   \\
**Jiantao Lin\***, Xin Yang\*, Meixi Chen\*, Yingjie Xu, Dongyu Yan, Leyi Wu, Xinli Xu, Lie Xu, Shunsi Zhang, Ying-Cong Chen

[**Project Page**](https://ltt-o.github.io/Kiss3dgen.github.io) | [**Code**](https://github.com/EnVision-Research/Kiss3DGen) | [**Demo**](https://envision-research.hkust-gz.edu.cn/kiss3dgen/)
  - This work explores utilizing 2D diffusion models for 3D asset generation.
  - Improves quality and diversity in 3D shape synthesis.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2025</div><img src='images/DiMeR_teaser.png' alt="DiMeR" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

#### **DiMeR: Disentangled Mesh Reconstruction Model**  
[**üìÑ Paper (arXiv)**](https://arxiv.org/pdf/2504.17670)   \\
Lutao Jiang\*, **Jiantao Lin\***, Kanghao Chen\*, Wenhang Ge\*, Xin Yang, Yifan Jiang, Yuanhuiyi Lyu, Xu Zheng, Yinchuan Li, Yingcong Chen

[**Project Page**](https://lutao2021.github.io/DiMeR_page) | [**Code**](https://github.com/lutao2021/DiMeR) | [**Demo**](https://huggingface.co/spaces/LTT/DiMeR)
  - Proposes DiMeR, a geometry-texture disentangled model with 3D supervision for improved sparse-view mesh reconstruction.
  - Enhances reconstruction accuracy and efficiency by separating inputs and simplifying mesh extraction.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2025</div><img src='images/kiss_plus_teaser.png' alt="kiss++" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

#### **Advancing high-fidelity 3D and Texture Generation with 2.5D latents**  
[**üìÑ Paper (arXiv)**](https://arxiv.org/pdf/2505.21050)   \\
Xin Yang\*, **Jiantao Lin\***, Yingjie Xu, Haodong Li, Yingcong Chen
  - Proposes a unified framework for joint 3D geometry and texture generation using versatile 2.5D representations.
  - Improves coherence and quality in text- and image-conditioned 3D generation via 2D foundation models and a lightweight 2.5D-to-3D decoder.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2025</div><img src='images/FlexPainter_teaser.png' alt="FlexPainter" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

#### **FlexPainter: Flexible and Multi-View Consistent Texture Generation**  
[**üìÑ Paper (arXiv)**](https://arxiv.org/pdf/2506.02620)   \\
Dongyu Yan\*, Leyi Wu\*, **Jiantao Lin**, Luozhou Wang, Tianshuo Xu, Zhifei Chen, Zhen Yang, Lie Xu, Shunsi Zhang, Yingcong Chen

[**Project Page**](https://starydy.xyz/FlexPainter) | [**Code**](https://github.com/StarRealMan/FlexPainter)
  - Proposes FlexPainter, a multi-modal diffusion-based pipeline for flexible and consistent 3D texture generation.
  - Enhances control and coherence by unifying input modalities, synchronizing multi-view generation, and refining textures with 3D-aware models.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2025</div><img src='images/prm_teaser.png' alt="prm" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

#### **PRM: Photometric Stereo-based Large Reconstruction Model**  
[**üìÑ Paper (arXiv)**](https://arxiv.org/pdf/2412.07371)   \\
Wenhang Ge*, **Jiantao Lin\***, Jiawei Feng, Guibao Shen, Tao Hu, Xinli Xu, Ying-Cong Chen

[**Project Page**](https://wenhangge.github.io/PRM/) | [**Code**](https://github.com/g3956/PRM) | [**Demo**](https://huggingface.co/spaces/LTT/PRM)
  - A large-scale photometric stereo reconstruction framework.
  - Enhances lighting-aware 3D shape recovery with high fidelity.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/luciddreamer_teaser.jpg' alt="luciddreamer" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

#### **LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching**  
[**üìÑ Paper (arXiv)**](https://arxiv.org/abs/2311.11284) <span class='show_paper_citations' data='ri-snP0AAAAJ:u5HHmVD_uO8C'></span>   \\
Yixun Liang\*, Xin Yang\*, **Jiantao Lin**, Haodong Li, Xiaogang Xu, Ying-Cong Chen

[**Code**](https://github.com/EnVision-Research/LucidDreamer)  
  - Proposes a novel interval score matching approach for text-to-3D generation.
  - Significantly improves 3D shape realism and fidelity.
</div>
</div>

## 2D Generation

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2025</div><img src='images/flexgen_teaser.png' alt="flexgen" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

#### **FlexGen: Flexible Multi-View Generation from Text and Image Inputs**  
[**üìÑ Paper (arXiv)**](https://arxiv.org/abs/2410.10745)   \\
Xinli Xu\*, Wenhang Ge\*, **Jiantao Lin\***, Jiawei Feng, Lie Xu, Hanfeng Zhao, Shunsi Zhang, Ying-Cong Chen

[**Project Page**](https://xxu068.github.io/flexgen.github.io/)  
  - A multi-view generation model that fuses text and image inputs.
  - Enables controllable and high-quality multi-view synthesis.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2025</div><img src='images/sg_teaser.jpg' alt="sg-adapter" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

#### **SG-Adapter: Enhancing Text-to-Image Generation with Scene Graph Guidance**  
[**üìÑ Paper (arXiv)**](https://arxiv.org/abs/2405.15321)   \\
Guibao Shen\*, Luozhou Wang\*, **Jiantao Lin**, Wenhang Ge, Chaozhe Zhang, Xin Tao, Yuan Zhang, Pengfei Wan, Zhongyuan Wang, Guangyong Chen, Yijun Li, Ying-Cong Chen.
  - Introduces scene graph constraints into text-to-image generation.
  - Improves structure and semantic consistency in generated images.
</div>
</div>

## 2D Recognition

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Pattern Recogn. Lett 2024</div><img src='images/webly_teaser.jpg' alt="webly-fg" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

#### **Graph Representation and Prototype Learning for Webly Supervised Fine-Grained Image Recognition**  
[**üìÑ Paper (Pattern Recognition Letters)**](https://www.sciencedirect.com/science/article/abs/pii/S0167865524001405)   \\
**Jiantao Lin**, Tianshui Chen, Ying-Cong Chen, Zhijing Yang, Yuefang Gao
  - Proposes a novel graph-based method for fine-grained image recognition under weak supervision.
  - Learns better category structures for challenging datasets.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICPR 2022</div><img src='images/CGLRL_teaser.png' alt="global-local" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

#### **Learning Consistent Global-Local Representation for Cross-Domain Facial Expression Recognition**  
[**üìÑ Paper (ICPR 2022)**](https://ieeexplore.ieee.org/abstract/document/9956069)   \\
Yuhao Xie, Yuefang Gao, **Jiantao Lin**, Tianshui Chen

[**Code**](https://github.com/yao-papercodes/CGLRL)  
  - Introduces a domain-adaptive method for cross-domain facial expression recognition.
  - Bridges the gap between different facial datasets through global-local feature alignment.
</div>
</div>